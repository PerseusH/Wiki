## Basic

- 字节(Byte)，一字节等于8位
- 1TB等于1024GB
- 处理器位数指**通用寄存器，内存寻址，和指令指针**的位数
- 32位处理器一条指令占据**4字节**
- 64位处理器一条指令占据**8字节**
- 如果我们将总长**128位的指令**分别按照16位、32位、64位为单位进行编辑的话：旧的16位处理器，比如Intel 80286 CPU需要**8个指令**，32位的处理器需要**4个指令**，而64位处理器则只要**两个指令**，显然，在**工作频率相同**的情况下，**64位处理器**的处理速度会比16位、32位的**更快**（在运行32位应用时，64位处理器和32位处理器处理速度一样，同一时间只能解码一组数据，所以不会出现64位处理器比32位快一倍现象。64位系统中存在一部分64位文件，在读取时反而会比32位系统慢）
- **64位处理器**的优势还体现在系统对**内存**的控制上。由于地址使用的是特殊的整数，而64位处理器的一个ALU（算术逻辑运算器）和寄存器可以处理更大的整数，也就是**更大的地址**。**32位处理器的寻址空间最大为3.2G**，使得很多需要大容量内存的数据处理程序在这时都会显得捉襟见肘，形成了运行效率的瓶颈。而**64位的处理器的寻址空间在理论上可达1700万TB**，所以64位的处理器能够彻底解决32位计算系统所遇到的瓶颈现象，速度快人一等，对于要求多处理器可扩展性、更大的可寻址内存、视频/音频/三维处理或较高计算准确性的应用程序而言，64处理器可提供卓越的性能

## 时钟周期

- 计算机中最基本的、**CPU工作的最小的时间单位**。在一个时钟周期内，**CPU仅完成一个最基本的动作**。**更小的时钟周期**就意味着**更高的工作频率**
- CPU内部的逻辑门电路需要一小段时间**对输入的变化做出反应**，所以**需要时钟周期来容纳消除传播时延**（propagation delay），这样才能**输出稳定的信号**。时钟周期应当大到能容纳所有逻辑门的传播时延
- **指令周期是取出并执行一条指令的时间**。指令周期常常有**若干个CPU周期**，CPU周期也称为机器周期，由于CPU访问一次内存所花费的时间较长，因此通常用**内存中读取一个指令字的最短时间**来规定CPU周期。这就是说，一条指令取出阶段（通常为取指）需要一个CPU周期时间。而**一个CPU周期又包含若干个时钟周期**。这些时钟周期的总和则规定了一个CPU周期的时间宽度
  - 处理一条指令需要的时钟周期不固定，这得看处理器的流水线级数，也得看该指令的复杂度，**在执行阶段需要几个时钟周期**

## 程序计数器

- 用来存储cpu当前执行的指令位置，或者即将执行的下一条指令位置

## 指令管线(Pipeline)

- **译码，执行和退出**三级流水线(管线)组成了X86处理器指令执行的基本模式。从最初的8086处理器到最新的酷睿i7处理器都基本遵循了这样的过程。虽然更新的处理器增加了更多的流水级，但基本的模式没有改变

  #### 流水线起源

  - 流水线概念来源于工业制造领域，以汽车装配为例来解释流水线的工作方式，假设装配一辆汽车需要四个步骤：

    第一步冲压：制作车身外壳和底盘等部件。

    第二步焊接：将冲压成形后的各部件焊接成车身。

    第三步涂装：将车身等主要部件清洗、化学处理、打磨、喷漆和烘干。

    第四步总装：将各部件（包括发动机和向外采购的零部件）组装成车。

    汽车装配则同时对应需要冲压、焊接、涂装和总装四个工人。最简单的方法是一辆汽车依次经过上述四个步骤装配完成之后，下一辆汽车才开始进行装配，最早期的工业制造就是采用的这种原始的方式，即同一时刻只有一辆汽车在装配。不久之后人们发现，**某个时段中一辆汽车在进行装配时，其它三个工人都处于闲置状态**，显然这是对资源的极大浪费，于是思考出能有效利用资源的新方法，即在第一辆汽车经过冲压进入焊接工序的时候，立刻开始进行第二辆汽车的冲压，而不是等到第一辆汽车经过全部四个工序后才开始，这样在后续生产中就能够**保证四个工人一直处于运行状态**，不会造成人员的闲置。这样的生产方式就好似流水川流不息，因此被称为流水线

  #### 经典五级流水线

  - 指令的执行周期分为五个单独的过程：**取指，译码，执行，访存，回写**

  - 前一条指令在完成了“取指”进入“译码”阶段后，下一条指令马上就可以进入“取指”阶段，依次类推，如果流水线没有停顿，理论上每个时钟周期都能完成一条指令

- **单级流水线**会面临一个重大的问题，即五个单独的过程中，有一个过程如果耗时太长，会导致个**流水线堵塞**，非常影响效率。因此，**CPU设计引入了多级流水线(多管线)**，不会因为一级堵塞，导致整个流水线堵塞。随着CPU技术的发展，流水线级数最大已经有31级。流水线的级数越多，意味着流水线被切的越细，每一级流水线容纳的硬件逻辑越少

  - 在两级寄存器（每一级流水线由寄存器组成）之间的**硬件逻辑越少**，意味能够运行到**更高的主频**。多管线**提高了CPU主频**，同时也带来了高能耗。**主频越高**意味着流水线的**吞吐率越高**从而性能越高

  #### 分支预测(Branch Prediction)

  - 预测 ***if*** 这样的分支语句将会执行哪个分支，解决处理分支指令（if-else)导致**流水线失败指令被清空**的问题
    - 随着流水线级数的增加，对流水线代码执行的优化变得可能了。流水化中的单元分的更详细， 更多的指令可以并行处理，但速度不见得快，因为有分支的出现，如果**没有命中第一个分支，后面的指令将作废**, 需要清空后面所有的指令, 然后**载入命中地址的指令**，再运行
    - 当包含流水线技术的处理器处理分支指令时就会遇到一个问题，根据判定条件的真/假的不同，有可能会产生**跳转**，而这会**打断流水线中指令的处理**，因为**处理器无法确定该指令的下一条指令**，直到分支指令执行完毕。流水线越长，**处理器**等待的时间便越长，因为它**必须等待分支指令处理完毕**，才能确定下一条进入流水线的指令
  
- if else  在执行时考虑到效率的原因，**不会按代码的顺序**一条一条去试，而是**随机找出一个分支执行**，如果不对，再随机找到一个分支
  
  - 分支地址只有在流水线指令执行阶段才能计算出来，为了避免等待，需要**在译码阶段进行预测**
  
  - 任何一种分支预测策略的效果都取决于该**策略本身的精确度和条件分支的频率**
  
  - **静态分支预测**：由编译器随机选择一个分支参与条件判断，一般是第一个分支，即 if 后面的逻辑，而不是后面else的逻辑。更精确的办法是根据原先运行的结果进行统计从而尝试预测分支是否会跳转
  
  - **动态分支预测**：在CPU硬件中开辟一块缓存，专门记录每个分支**最近几次的命中情况**，然后做出预测，显然这种方法能及时调整策略，有更好的前瞻性，但CPU压力会大些，不过还能接受。这就是**根据历史做出的预测**
    
    - 如果过去 99% 的次数都是在某个分支执行，那么 CPU 就会猜测：下一次可能还会在此分支执行，因此可以提前将这个分支的代码装载到流水线上。如果预测失败，则需要清空流水线并重新加载，可能会损失 20 个左右的时钟周期时间
    - `分支目标缓冲(BTB)`记录分支的目标即**跳转地址**的缓冲
    - `分支历史表(BHT)`记录分支历史信息
    - 基于BTB和BHT的分支预测就很简单了：
      1）在取指阶段利用PC寻址BTB，如果命中，则说明这是一条跳转指令，利用从BTB中获取到的地址去取icache；
      2）由于BTB中保存的内容不够多，因此**BHT的准确率更高**，这个时候索引BHT表格，如果发现BHT也跳转，则说明这条指令预测是跳转的；如果BHT不跳转，则说明不跳转，这个时候就取消BTB中的指令地址，重新PC+4去取icache
    
  - ```
    //为什么有序数组比无序数组快???
    for (int c = 0; c < arraySize; ++c)
    {
        if (data[c] >= 128)
        sum += data[c];
    }
    //注意到data这个数组里面的值是平均分布于0-255之间的。当***数据排好序***之后，前半部分的数据都会小于128，所以不会进到if语句里面，后半部分的值都大于128，所以会进到循环语句
    //这种规律对于分支预测器***非常友好***，因为分支判断语句发现***总是选择某个方向很多次***，于是就很容易做出判断。即使一个简单的计数器就可以正确预测分支的方向，除了改变方向之后的一两次预测失败之外
    
    //将下面的分支语句
    if (data[c] >= 128)
        sum += data[c];
    //替换为
    int t = (data[c] - 128) >> 31;
    sum += ~t & data[c];
    //这里使用了***位运算***，取代移除了分支预测
    //在没有分支判断的条件下，对有序和无序数组的处理，在速度上是差不多的, 和使用分支预测的有序数组速度也差不多
    ```
    
  - Switch对于CPU来说难以做分支预测；某些Switch条件如果**命中概率比较高**，可以针对此高频switch分支**在switch前提前进行if判断，将此分支从switch中单独抽离到if分支**，充分利用CPU的分支预测机制

## MMU(Memory Management Unit)

- 内存管理单元，负责处理中央处理器（CPU）的**内存访问请求**的控制线路。功能包括虚拟地址到物理地址的转换（即虚拟内存管理，重定向）、提供硬件机制的内存访问授权(内存保护)、中央处理器高速缓存的控制
- 在使用了虚拟存储器的情况下，虚拟地址不是被直接送到内存地址总线上，而是送到存储器管理单元MMU，把虚拟地址映射为物理地址
- 多用户多进程操作系统，需要MMU，才能实现每个用户进程都拥有自己**独立的地址空间**
- MMU能使单个软件线程工作于硬件保护地址空间。当应用程序的所有线程共享同一内存空间时，任何一个线程将有意或无意地破坏其它线程的代码、数据或堆栈。异常线程甚至可能破坏内核代码或内部数据结构。例如线程中的指针错误就能轻易使整个系统崩溃，或至少导致系统工作异常
- 就安全性和可靠性而言，基于进程的实时操作系统(RTOS）的性能更为优越。为生成具有单独地址空间的进程，RTOS只需要生成一些基于RAM的数据结构并使MMU加强对这些数据结构的保护。基本思路是在每个关联转换中“接入”一组新的逻辑地址。MMU利用当前映射，将在指令调用或数据读写过程中使用的逻辑地址映射为存储器物理地址。MMU还标记对非法逻辑地址进行的访问，这些非法逻辑地址并没有映射到任何物理地址
-  这些进程虽然增加了利用查询表访问存储器所固有的系统开销，但其实现的效益很高。在进程边界处，疏忽或错误操作将不会出现，用户接口线程中的缺陷并不会导致其它更关键线程的代码或数据遭到破坏
- 采用MMU还有利于选择性地将页面映射或解映射到逻辑地址空间。物理页帧映射至逻辑空间，以保持当前进程的代码，其余页面则用于数据映射。类似地，**物理页帧通过映射可保持进程的线程堆栈**。RTOS可以在每个线程堆栈解映射之后，很容易地保留逻辑地址所对应的页帧内容。这样，如果任何线程分配的堆栈发生溢出，将产生硬件存储器保护故障(**抛出异常**)，内核将挂起该线程，而不使其破坏位于该地址空间中的其它重要存储器区，如另一线程堆栈。这不仅在线程之间，还在同一地址空间之间增加了存储器保护
- 内存保护（包括堆栈溢出检测）在应用程序开发中通常非常有效。采用了内存保护，程序错误将**产生异常并能被立即检测**，它由源代码进行跟踪。如果没有内存保护，程序错误将导致一些细微的难以跟踪的故障。实际上，由于在扁平存储器模型中，RAM通常位于物理地址的零页面，因此甚至NULL指针引用的解除都无法检测到

## 保护模式

- 80286系列和之后的x86兼容CPU操作模式。保护模式有一些新的特色，设计用来增强多工和系统稳定度，像是 **内存保护，分页 系统，以及硬件支援的 虚拟内存**。大部分的现今 x86 操作系统 都在保护模式下运行，包含 Linux、FreeBSD、以及 微软 Windows 2.0 和之后版本

## 实模式

- **向前兼容**且关闭了保护模式这些特性的CPU运行模式。用来让新的芯片可以运行旧的软件。依照设计的规格，所有的x86 CPU都是**在实模式下开机**，来确保传统操作系统的向前兼容性。在任何保护模式的特性可用前，他们必须要由某些程序手动地切换到保护模式。在现今的计算机，这种**切换**通常是由操作**系统开机必须完成的第一件任务的一个**。它也可能当CPU在保护模式下运行时，使用虚拟86模式来运行设计运行在实模式下的代码

## 回调函数

- 通过函数指针调用的函数。如果你把函数的指针（地址）作为参数传递给另一个函数，当这个指针被用为调用它所指向的函数时，我们就说这是回调函数

## 信号

- 信号是进程对内核软中断的模拟，异步的进程间通讯机制，相应的处理函数在进程的**用户空间**里执行