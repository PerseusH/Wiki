```
fd = open (name, flag, mode);
if(fd<0)
{
	printf("error!\n");
}
        
/* 获取共享内存指针，这块内存可读可写可执行，用户进程可以像操作内存一样操作文件 */
ptr = mmap(NULL, len , PROT_READ|PROT_WRITE|PROT_EXEC, MAP_SHARED , fd , 0);
```

- 传统的 Linux 系统的**标准 I/O 接口（read、write）是基于数据拷贝的**，也就是数据都是 copy_to_user 或者 copy_from_user，这样做的好处是，通过中间缓存的机制，减少磁盘 I/O 的操作，但是坏处也很明显，大量数据的拷贝，用户态和内核态的频繁切换，会**消耗大量的 CPU 资源**，严重影响数据传输的性能，有数据表明，在Linux内核协议栈中，这个拷贝的耗时甚至占到了数据包整个处理流程的57.1%

- 内存映射通过MMU建立的，可以从/proc/<pid>/maps看到每个进程的mmap状态，比如共享内存地址

- 将**page cache物理页**直接映射到用户进程地址空间(**进程内存堆与栈之间的虚拟地址空间**)中，也就是将硬盘文件映射到进程虚拟地址空间中

  - mmap映射的本质就是页缓存的**内核态**虚拟地址和进程的**用户态**虚拟地址指向了同一个物理内存页(**共享**)
  - 进程的**用户态虚拟地址空间**存储的是**映射关系**数据
  - 使用页缓存的普通IO为标准IO，需要把数据从页缓存拷贝到用户地址空间，mmap则只需将页缓存物理页映射到用户地址空间

- 实现映射关系后，进程就可以采用指针读写操作这一段内存，而系统会**自动回写脏页**到对应的文件磁盘上，完成对文件的操作而**不必**调用read,write等**系统调用**，只调用mmap()一次

- 普通文件IO和进程间通信需要复制两次，内存映射只需复制一次，**mmap减少了内存间数据复制过程和读写系统调用**

- 将文件的内容从页缓存拷贝到用户的空间这个过程和缺页异常(通过DMA从磁盘调入需要的页)不一样，这个拷贝过程需要通过CPU进行，因此**浪费了CPU时间**

- 当**大量数据**需要传输的时候，采用内存映射方式去访问文件会获得比较好的效率

- #### 劣势场景

  - 对于大文件，提高了IO效率；对于**小文件**，由于mmap执行了更多内核操作(多次通过**缺页中断陷入内核**拷贝文件数据到内存)，其效率可能比普通的文件IO更差
  - 如果**更新文件的操作很多**，mmap避免两态拷贝的优势就被摊还，最终还是落在了**大量的脏页回写**及由此引发的**随机IO**上。在**随机写很多**的情况下，mmap方式在效率上不一定会比带缓冲区的一般写快
  - **对变长文件不适合**

- mmap中进程可以和别的进程**共享**内存映射区中的数据，节约了内存

- 文件映射分为私有映射(private)和共享映射（shared)两种，二者之间的区别就是一个进程对文件所做的改变能否被其他的进程所看到，且能否同步到后备的存储介质中。**共享映射**所有的进程都是共享同一块缓存的，此时被映射的文件的数据**在内存中只保留一份**。任何一个进程对映射区进行读或者写，都不会导致对缓存数据的复制

- 对于输入来讲，一旦内核将相应的内存映射进用户内存之后，用户进程就能够使用这些数据了；对于输出来讲，用户进程仅仅需要修改共享内存(pagecache)中的内容，然后可以依靠内核**脏页回写机制**自动更新底层的文件

- mmap的主要的好处在于，减少一次内存拷贝。在我们平时vfs的read/write系统调用中，文件内容的拷贝要多经历内核页缓存这个阶段，所以比mmap多了一次内存拷贝，**mmap只有用户空间的内存拷贝**(这个阶段read/write也有）。正是因为**减少了从页缓存到用户内存的拷贝**，所以mmap大大提高了性能，mmap也被称为zero-copy技术

  #### 没有内存映射

  - 磁盘IO：用户进程发出IO请求，内核态先查询**页缓存**是否有对应文件或设备数据，页缓存有数据则直接返回拷贝至用户进程，没数据有则通过缺页中断**从磁盘拷贝数据至页缓存，再拷贝至用户进程**

  - 进程间通信：用户态发送进程发出数据，**通过系统调用拷贝至内核缓冲区，再通过内核缓冲区拷贝至接收进程**
  - 没有内存映射，磁盘IO和进程间通信都需要**两次数据拷贝**

  #### 使用内存映射

  - 磁盘IO：系统调用mmap()通过内存中的inode定位到磁盘文件物理地址，实现文件物理地址到进程虚拟内存区的映射。只有在进程发起**IO请求**时，才会为该文件分配物理内存(**页缓存**)并建立页表映射，使用缺页中断**将文件数据从设备驱动直接拷贝到页缓存**
    - 在**用户进程的虚拟地址空间**开辟一段虚拟内存区，连真实的物理地址都不会分配，当你访问这段空间，CPU陷入内核执行缺页异常处理，然后异常处理程序会在这个时间分配物理内存，并用文件的内容填充这片内存，然后返回进程的上下文，这时程序才会感知到这片内存里有数据
  - 进程间通信：在进程间创建一块**进程间共享的用户态内存缓冲区**，并将发送进程和接收进程的虚拟地址空间同时映射到这块缓冲区。只有在发送进程**发送数据**时，才会**将数据拷贝至共享缓冲区**内，此时，接收进程可直接访问共享缓冲区数据，实现了跨进程通信
    - mongoDB （3.0以前版本） 、rocketMQ 都用到了 mmap
  - 使用了内存映射后，磁盘IO和进程间通信只需要**一次数据拷贝**

- 内存的最小粒度是页，而进程虚拟地址空间和内存的映射也是以页为单位。为了匹配内存的操作，mmap从磁盘到虚拟地址空间的映射也必须是页

- 映射建立之后，即使**文件关闭，映射依然存在**

- 除了节省内核空间和用户空间之间的一次传输之外，mmap()还能够通过减少所需使用的内存来提升性能。当使用read()或write()时，数据将被保存在两个缓冲区中：一个位于用户空间，另个一位于内核空间。当使用mmap()时，内核空间和用户空间会共享同一个缓冲区。此外，如果**多个进程**正在**同一个文件**上执行I/O，那么它们通过使用mmap()就能够**共享同一个内核缓冲区**，从而又能够**节省内存**

- 进程发起网络写请求时，也会通过共享的页缓存**将进程数据拷贝到内核socket缓冲区**，最后再**通过DMA拷贝到网卡**发送出去

- **RocketMQ**的消息采用顺序写到commitlog文件，然后利用consume queue文件作为索引；RocketMQ采用零拷贝mmap+write的方式来回应Consumer的请求