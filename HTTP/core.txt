1.一次完整的http请求全过程
    1.浏览器输入url
    2.DNS域名解析
    3.tcp3次握手建立连接
    4.发送http请求
    5.服务器程序按照请求生成响应结果返回给客户端(浏览器)(短连接马上tcp4次挥手关闭连接)
    6.浏览器解析html代码，并请求其中的资源
    7.浏览器渲染并显示页面

2.tcp/udp的区别？tcp粘包是怎么回事,如何处理？udp有粘包吗？
    |保护消息边界 -> 把每条数据当作'{独立的消息}'传输,接收端一次只能接收一个数据包
    |流传输 -> '{无保护消息边界}',如果发送端连续发送数据,接收端可能一次收到多个数据包

    |TCP -> 基于连接(需三次握手建立),对系统资源要求较多,包头结构复杂,保证数据正确性和顺序
        -> TCP为了保证可靠传输,'{减少额外开销}'(每次发包都要验证),采用了'{流传输}',相对于面向消息的传输,可以减少包的发送量,减少开销

        |TCP报文字段
            |序号(seq) -> 随机数或接收到的ack_seq, 用来保证数据包顺序
            |确认序号(ack_seq) -> 接收到的seq + 1
            |标志位 -> SYN(0: 无询问, 1: 询问), ACK(0: 无应答, 1: 确认), FIN(0: 未关闭, 1: 关闭)

        |三次握手 -> 建立TCP连接, 分配资源、初始化序列号(通知对方我的初始序列号是多少)
            -> 为了防止已失效的连接请求报文段突然又传送到了服务端，因而产生错误
                -> client发出的第一个连接请求报文段并没有丢失，而是在某个网络结点长时间的滞留了，以致延误到连接释放以后的某个时间才到达server。本来这是一个早已失效的报文段。但server收到此失效的连接请求报文段后，就误认为是client再次发出的一个新的连接请求。于是就向client发出确认报文段，同意建立连接。假设不采用“三次握手”，那么只要server发出确认，新的连接就建立了。由于现在client并没有发出建立连接的请求，因此不会理睬server的确认，也不会向server发送数据。但server却以为新的运输连接已经建立，并一直等待client发来数据。这样，server的很多资源就白白浪费掉了。采用“三次握手”的办法可以防止上述现象发生。例如刚才那种情况，client不会向server的确认发出确认。server由于收不到确认，就知道client没有要求建立连接
            client(SYN) -> server(ACK+SYN) -> client(ACK)

        |四次挥手 -> 终止TCP连接, 释放连接资源. 四次挥手是因为被动关闭方接收到FIN时需要处理完未发送的报文后才能确定关闭连接
            -> TCP是全双工模式，这就意味着，当主机1发出FIN报文段时，只是表示主机1已经没有数据要发送了，主机1告诉主机2，它的数据已经全部发送完毕了；但这时主机1还是可以接受来自主机2的数据；当主机2返回ACK报文段时，表示它已经知道主机1没有数据发送了，但是主机2还是可以发送数据到主机1的；当主机2也发送了FIN报文段时，这个时候就表示主机2也没有数据要发送了，就会告诉主机1，我也没有数据要发送了，之后彼此就会愉快地中断这次TCP连接
            |两端同时关闭连接 -> 一个关闭方发出FIN时接收到另一方发过来的FIN, 则双方同时进入FIN_WAIT; 在确认收到了对方全部的数据包后, 双方返回ACK, 然后同时进入TIME_WAIT

            client(FIN) -> server(ACK), server(FIN) -> client(ACK)

            Client                                              Server
              |--------------------- Connect ---------------------|
              |          --SYN=1,seq=m,ACK=0,ack_seq=0->          |
      SYN_SENT|                                                   |
              |         <-ACK=1,ack_seq=m+1,SYN=1,seq=n--         |
              |                                                   |SYN_RCVD
              |        --ACK=1,ack_seq=n+1,SYN=0,seq=m+1->        |
   ESTABLISHED|                                                   |ESTABLISHED
              |--------------------- DataSend --------------------|
              |     --ACK=1,ack_seq=n+1,SYN=0,seq=m+1,data1->     |
              |      <-ACK=1,ack_seq=m+1+L_d1,SYN=0,seq=n+1--     |
              |   --ACK=1,ack_seq=n+2,SYN=0,seq=m+1+L_d1,data2->  |
              |   <-ACK=1,ack_seq=m+1+L_d1+L_d2,SYN=0,seq=n+2--   |
              |---------------------- Close ----------------------|
   ESTABLISHED|                                                   |ESTABLISHED
              |   --FIN=1,seq=m+1+L_d1+L_d2,ACK=1,ack_seq=n+2->   |
      FIN_WAIT|                                                   |
              |          <-ACK=1,ack_seq=m+2+L_d1+L_d2--          |
              |                                                   |CLOSE_WAIT
              |   <-FIN=1,seq=n+2,ACK=1,ack_seq=m+2+L_d1+L_d2--   |
              |                                                   |LAST_ACK
              |               --ACK=1,ack_seq=n+3->               |
     TIME_WAIT|                                                   |
        CLOSED|                                                   |CLOSED

        |TCP粘包 -> 只在'{流传输}'中出现
        -> 客户端与服务器会维持一个连接(Channel),数据在连接不断开的情况下,可以持续不断地将多个数据包发往服务器.如果发送的数据包太小,那么会启用Nagle算法(可配置是否启用)'{对较小的数据包进行合并}'(因此'{TCP的网络延迟比UDP的高}')然后再发送(缓冲区满、超时或者包大小足够).这样服务器在接收到数据流的时候就无法区分哪些数据包是客户端自己分开发送的,就产生了粘包;如果服务器消息没有被及时从缓存区取走,下次在取数据的时候可能就会出现'{一次取出多个数据包}'的情况,导致粘包(确切来讲,对于基于TCP协议的应用,不应用包来描述,而应用流来描述)
            |解决办法
            1.对于发送方引起的粘包,用户可通过编程设置来避免,TCP提供了'{强制数据立即传送}'的指令push,服务器收到push指令后,将本段数据'{即时发送}'出去,而不必等待发送缓冲区满
            2.对于接收方引起的粘包,可通过优化程序设计、精简接收进程工作量、提高接收进程优先级等措施,使其'{及时接收}'数据,从而尽量避免出现粘包现象
            3.封装待传输数据包时,用固定'{分隔符}'作为结尾符(数据中不能含结尾符),这样我们接收到数据后,如果出现结尾符,即人为将粘包分开;如果包中没有出现结尾符,则认为是分包,等待下个包中出现结尾符时合并成一个完整的包(常用于文本传输数据,如采用/r/n之类的分隔符)
            4.在数据包的固定位置封装'{数据包的长度信息}'(或可计算数据包总长度的信息),服务器接收到数据后,先是解析包长度,然后根据包长度截取数据包(常用于自定义协议中)

    |UDP -> 无连接,系统资源需求少,包头结构简单,不保证数据正确性和顺序,面向'{数据报}'传输
        -> 作为无连接不可靠的传输协议(适合频繁发送较小包),有'{保护消息边界}','{不会对数据包进行合并}',数据直接发出去,因此'{不会出现粘包}',每一个数据包也都是完整的(数据+UDP头+IP头等等发一次数据封装一次)

3.time_wait是什么情况？出现过多的close_wait可能是什么原因？
    |RST分节 -> 在TCP协议中RST表示复位,用来异常地关闭连接,在TCP的设计中它是不可或缺的。发送RST包关闭连接时,不必等缓冲区的包都发出去,直接就丢弃缓存区的包发送RST包。而接收端收到RST包后,也不必发送ACK包来确认。系统收到RST包就会产生一个错误然后'{立即关闭连接}'

    |MSL(MaximumSegmentLifetime,最大分节生命期) -> 一个数据包在网上的'{最长生存时间}',超过这个时间数据包将消失.RFC1122建议是2分钟,berkeley的TCP实现传统上是30秒

    |TIME_WAIT -> 主动发送出FIN信号后又接收到对方发来的FIN信号, 就会进入TIME_WAIT状态
        -> TIME_WAIT状态维持时间是'{两个MSL}'时间长度,即1-4分钟.Windows系统是4分钟
        -> 通信双方建立TCP连接后,'{主动关闭连接}'的一方会进入TIME_WAIT状态
        -> 主动关闭连接的一端会发送最后一个ack,然后进入TIME_WAIT状态,停留2个MSL时间后进入CLOSED状态

        |作用
            1.可靠地实现TCP全双工连接的终止(保证可重发丢失的最终ACK)
            -> TCP协议在关闭连接的四次握手过程中,最终的ACK是由主动关闭连接的一端(后面统称A端)发出的,如果这个'{ACK丢失}',对方(后面统称B端)将重发出最终的FIN,因此A端必须维护状态信息(TIME_WAIT)允许它'{重发最终的ACK}'.如果A端不维持TIME_WAIT状态,而是处于CLOSED 状态,那么A端将发送'{RST分节}',B端收到后将此分节解释成一个错误(在java中会抛出connection reset的SocketException)
            因此要实现TCP全双工连接的正常终止,必须处理终止过程中四个分节任何一个分节的丢失情况,主动关闭连接的A端必须维持TIME_WAIT状态
            2.允许旧的重复分节在网络中消失(保证旧的重复分节完全被释放后才会重新连接)
            -> TCP分节可能由于路由器异常而"迷途",在迷途期间,TCP发送端可能因确认超时而重发这个分节,迷途的分节在路由器修复后也会被送到最终目的地,这个迟到的迷途分节到达时可能会引起问题.在关闭前一个连接之后,马上又重新建立起一个相同的IP和端口之间的新连接,前一个连接的迷途重复分组在前一个连接终止后到达,而被新连接收到了.为了避免这个情况,TCP协议'{不允许TIME_WAIT状态的连接启动一个新连接}',因为TIME_WAIT状态持续2MSL,就可以保证当成功建立一个新TCP连接的时候,来自旧连接重复分组已经在网络中消失

        |后端server出现大量TIME_WAIT的情况 -> 主要是因为短连接
            -> TIME_WAIT一般持续1-4分钟, 会严重消耗服务器资源, 大量消耗客户端端口

            |解决方法
                -> nginx没有打开和后端的'{长连接}'，即：没有设置proxy_http_version 1.1;和proxy_set_header Connection “”;从而导致后端server每次关闭连接，高并发下就会出现server端出现大量TIME_WAIT
                -> Linux下开启tcp_tw_recycle+tcp_timestamps, 快速回收TIME_WAIT
                -> Linux下开启tcp_tw_reuse+tcp_timestamps, TIME_WAIT状态的socket连接可以'{被新连接重用}'
                -> tcp_max_tw_buckets'{控制并发的TIME_WAIT数量}'，默认值是180000。如果超过默认值，内核会把多的TIME_WAIT连接清掉，然后在日志里打一个警告。官网文档说这个选项只是为了阻止一些简单的DoS攻击，平常不要人为降低它
                -> 利用向目标发送RST包清理TIME_WAIT(RST攻击)

    |过多close_wait -> 被动关闭方(接收到对方发过来FIN信号)未关闭socket造成
        |原因 -> 在服务器与客户端通信过程中,因服务器未关闭socket导致closed_wait发生,致使监听port打开的句柄数到了1024个(每个用户默认分配的句柄上限),且均处于close_wait的状态.close_wait状态的socket需要先发送待处理的报文，然后才会关闭连接，这样的socket大量出现时，就会阻塞服务器。最终造成配置的port被占满出现“Too many open files”,无法再进行通信
        -> linux在文件句柄的数目上有两个级别的限制('{系统级限制和针对用户的限制}').一般情况1024够用了,但是在频繁使用网络和文件IO的大容量系统上,句柄很快就会耗光
        -> 有两种限制soft&hard,数目超过soft时会warning,达到hard时系统将拒绝或异常

        |解决办法
            1.设置超时 -> 调用ServerSocket类的accept()方法和Socket输入流的read()方法时会引起线程阻塞,所以应该用setSoTimeout()方法设置超时(缺省的设置是0,即超时永远不会发生);超时的判断是累计式的,一次设置后,每次调用引起的阻塞时间都从该值中扣除,直至另一次超时设置或有超时异常抛出
            比如,某种服务需要三次调用read(),超时设置为1分钟,那么如果某次服务三次read()调用的总时间超过1分钟就会有异常抛出,如果要在同一个Socket上反复进行这种服务,就要在每次服务之前设置一次超时

            2.修改配置
                |增加 -> /etc/security/limits.conf #保存后reboot
                    username(* -> 所有用户) soft nofile 65535 #软限制
                    username hard nofile 65535 #硬限制

                |系统级句柄数限制修改
                    sysctl -w fs.file-max 65536
                 or echo "65536" > /proc/sys/fs/file-max
                 or echo "fs.file-max=65536" >> /etc/sysctl.conf #修改内核参数

                |优先级 -> soft<hard<kernel<最大fd数采用的数据结构导致的限制

            3.使用'{连接池}'来控制连接数
