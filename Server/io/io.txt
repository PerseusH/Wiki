|IO操作 -> 获取IO结果数据, 然后从内核态IO线程拷贝数据到用户态进程

#### fd 

- socket文件描述符. 是一个指向文件的引用

- Socket起源于Unix. Unix中把所有的资源都看作是文件，比如网卡、打印机等，所以Socket通信使用网卡，网卡又处理许多连接，每个连接都需要一个对应的ID，即对应的文件描述符。函数socket()返回的就是这个描述符。在传输中要使用这个惟一ID来确定要往哪个连接上传输数据
      > int fd = socket(AF_INET,SOCK_STREAM, 0);
  - 可用系统调用 ioctl 将 fd 设成非堵塞

|同步IO
    -> 在一个线程中,CPU执行代码的速度极快,一旦遇到IO操作,如读写文件,发送网络数据时,就需要等待IO操作完成,才能继续进行下一步操作
        > f = open('/path/to/file','r')
        > r = f.read() #线程停在此处等待IO操作结果
        > #IO操作完成后线程才能继续执行
        > do_something(r)

|同步阻塞IO -> 当进程调用某些设计IO操作的系统调用或库函数时,比如accept()、send()、recv()等,就会阻塞自身,等待IO完成

|同步非阻塞IO
    -> 在非阻塞式IO中，用户进程需要'不断地主动询问'(recvfrom())kernel数据准备好了没有
        fcntl(fd, F_SETFL, O_NONBLOCK); //将fd设为非阻塞状态
    -> 当kernel中数据准备好的时候，recvfrom()会将数据从kernel'拷贝'到用户内存中，在这段时间内，进程是被阻塞的
    -> 同步非阻塞IO好处是在一个进程中可以同时处理多个IO操作,而不是阻塞在一个IO操作上
    -> 与同步阻塞IO相比, 多路复用和异步IO最大的优势是执行多任务时大大减少了线程数(内存开销)和频繁的线程上下文切换(CPU开销)

|异步IO -> 协程, AIO
    -> 用户进程发起IO操作(基于DMA)之后，立刻就返回执行其他IO. IO执行完毕时, 内核将数据复制到用户态, 当这些都完成后，kernel会向用户进程发送一个'signal'告诉它IO完成. 在此期间用户进程不需要去询问fd状态
    -> 异步IO不需要CPU进行任务调度, 多线程需要CPU进行调度
    -> 非阻塞IO和异步IO的主要区别在于非阻塞IO在IO进行期间会不停调用recvfrom()询问fd是否就绪
    |DMA(直接内存访问) -> 是一种硬件机制，它允许外围设备和内存之间直接传输IO数据，'不需要CPU'的参与。使用这种机制可以大大提高与设备通信的吞吐量. 高效的DMA处理依赖于'中断'报告
        -> 拥有DMA功能的硬件在和内存进行数据交换的时候可以不消耗CPU资源。只要CPU在发起数据传输时发送一个指令，硬件直接和内存交换数据，在传输完成之后硬件会触发一个中断来通知操作完成。这些无须消耗CPU时间的I/O操作正是异步操作的硬件基础
        |中断上下文
            1.中断上文 -> 硬件通过中断触发信号，导致'内核调用中断处理程序'，进入内核空间。这个过程中，硬件的一些变量和参数也要传递给内核，内核通过这些参数进行中断处理。中断上文可以看作就是硬件传递过来的这些参数和内核需要保存的一些其他环境(主要是当前被中断的进程环境)
            2.中断下文 -> 执行在内核空间的中断服务程序
            -> 中断又叫异步中断，由'硬件触发'。而异常又称为同步中断，由软件触发
            |中断服务程序（中断处理函数）-> 处理中断响应，遵循特定原型声明的C函数. 它运行在中断上下文中，也称为原子上下文，代码运行在此上下文中是'不能被阻塞'的。中断服务程序必须运行非常快，它最基本的工作就是向硬件确认已经收到中断信号，但通常还执行大量其他的工作。为此，一般中断服务程序分为两半，上半部是数据恢复处理函数，只执行可以很快执行的代码，如向硬件确认已经收到中断信号等，其他工作要延迟到下半部去执行
            执行在中断上下文中的代码需要注意的一些事项
                中断上下文中的代码不能进入休眠
                不能使用mutex，只能使用自旋锁，且仅当必须时
                中断处理函数不能直接与用户空间进行数据交换
                中断处理程序应该尽快结束
                中断处理程序不需要是可重入的，因为相同的中断处理函数不能同时在多个处理器上运行
            中断处理程序可能被一个优先级更高的中断处理程序所中断。为了避免这种情况，可以要求内核将中断处理程序标记为一个快速中断处理程序（将本地CPU上的所有中断禁用），不过在采取这个动作前要慎重考虑对系统的影响
            操作系统一般是通过中断从用户态切换到内核态。中断就是一个硬件或软件请求，要求CPU暂停当前的工作，去处理更重要的事情。比如，在x86机器上可以通过int指令进行软件中断，这个指令会导致一个异常：产生一个事件，这个事件会致使处理器切换到内核态并跳转到一个新的地址，并开始执行那里的异常处理程序。此时的异常处理程序实际上就是系统调用处理程序. 而在磁盘完成读写操作后会向CPU发起硬件中断
        -> Linux的IO调度器中有一个所有进程共享的公共异步IO任务队列
        -> 通过linux内核异步触发的IO调度（如：被时钟中断触发、被其他IO请求触发、等），已经提交的IO请求被内核的IO调度器调度，由对应的设备驱动程序提交给具体的设备。对于磁盘，驱动程序会发起一次DMA。经过若干时间，读写请求被磁盘处理完成，CPU将收到表示'DMA完成'的中断信号，设备驱动程序注册(在内核中)的中断处理函数将在'中断上下文'中被调用。这个处理函数会调用end_request函数来结束这次请求
            1.硬件产生中断，宣告新数据的到来
            2.中断处理程序分配一个缓冲区，并且告诉硬件向哪里传输数据
            3.外围设备将数据写入数据区，完成后产生另外一个中断
            4.处理程序分发新数据，唤醒相关进程，然后执行清理工作
    -> Linux: AIO(用用户态线程模拟, 不是真正的异步), Windows: IOCP(真正异步)
        |IOCP(IO Completion Port) -> 一个线程同步对象，有点像信号量(semaphore).一个IOCP对象和很多支持异步IO调用的IO对象相联系.线程有权阻塞IOCP对象，直到异步IO调用完成.高性能的异步IO模型，是一种应用程序使用线程池处理异步请求的机制
    -> epoll与iocp性能差异很小. 实际应用中，epoll对比iocp是有优势的
        1.IOCP读取数据操作，需要预先分配缓冲区，如果分配的缓冲区较小，会导致更多的系统调用，影响效率；如果分配的缓冲区大一些，会占用过多的内存
        2.epoll对应的reactor模型可以按需分配，及时释放，合理利用内存
        3.epoll的api比iocp简洁很多,一个完整的的epoll示例代码约100行,iocp的约200行
        4.从目前的服务器来看，linux比windows稳定许多

|事件驱动 -> Reactor模式
    -> 一般用主线程 + EventLoop + 消息队列(传递事件和回调)实现
    -> 当处理I/O或者其他耗时的操作时，注册一个回调到事件循环中('监听事件源')，然后当I/O操作完成时继续执行。回调描述了该如何处理某个事件。事件循环轮询所有的事件，将事件分配给相应的回调函数进行处理, 最后把处理结果推入消息队列等待主线程处理。这种方式让程序尽可能的得以执行而不需要用到额外的线程
    -> 在回调函数中，使用阻塞方法，会阻塞整个事件轮询
    |Node(IOCP + Evengloop)
        -> EventLoop是一个while(1)，用于等待和发送消息和事件。简单说，就是在程序中设置两个线程：一个负责程序本身的运行同时监听事件，称为"主线程"；另一个负责事件处理和回调操作，被称为"Eventloop线程". 不存在线程安全问题
            ReqWrap -> 主线程监听到事件发生, 封装回调函数和事件参数到请求对象, 随后将之推入线程池中等待多线程执行IO
                -> sys-cmd, webpage-opr, disk-IO, network-IO etc.

​        IO-Threads -> 通知IOCP对象操作已经完成，并将线程归还给线程池.在这一过程中事件循环会调用GetQueuedCompletionStatus方法检查IOCP对象中是否有执行完的请求，有则将请求对象加入到IO观察者队列(消息队列)中，将其作为事件处理

​        |Eventloop -> 判断观察者队列中是否有事件需要处理, 有则取出事件; 事件有回调则执行回调函数
​            -> 生产者/消费者模型
​            > while(1){
​            >   1.Get event from watchers
​            >   2.Execute callback of event if exists
​            >   3.Check IOCP and insert completed objs into watchers as events
​            > }

​            |Watcher -> 用来判断是否有事件需要处理(注册)。事件循环中有一到多个观察者，判断过程会向观察者询问是否有需要处理的事件
​                -> 类似于饭店的后厨与前台的关系。后厨每做完一轮菜，就会向前台经理询问是否还有要做的菜，如果有就继续做。服务员们相当于线程池, 前台经理就相当于观察者，他收到的顾客点单就是回调函数

## 多路复用(IO Multiplexing) 

- select/poll/epoll, 单进程处理多个IO
- 监听多个fd,一旦某个fd就绪(读就绪或写就绪),通知用户进程进行相应的读写操作
- **Nginx默认使用select/poll**
- 当被监听fd没有就绪的时候, select/poll/epoll处于阻塞状态, 因此多路复用属于**同步IO**
- 与同步非阻塞IO相比，IO复用模型的优势在于可以同时等待多个（而不是一个）fd就绪

#### 轮询 

- 使用poll函数去轮询，poll函数实际是阻塞的，内核buffer有数据就唤醒select/poll/epoll，一旦发现有数据就发起io调用，这就是通知机制
- 由于网络延迟，任一时间只有部分的socket是“活跃”的，但是**select/poll每次调用都会线性扫描全部的fd集合**，导致效率呈现线性下降。但是epoll不存在这个问题，它只会对“活跃”的socket进行操作---这是因为在内核实现中epoll是根据**每个fd上面的callback函数**实现的。**只有活跃socket才会主动的去调用 callback**，其他idle状态socket则不会，在这点上，epoll实现了一个“伪”AIO，因为这时候推动力在os内核。在一些 benchmark中，如果所有的socket基本上都是活跃的---比如一个**高速LAN环境，epoll并不比select/poll有什么效率**，相反，如果过多使用epoll_ctl,效率还有稍微的下降。但是一旦使用idle connections模拟WAN环境，epoll的效率就远在select/poll之上了

#### select

- 获取就绪fd时轮询,**遍历全部fd**,一个进程**能打开的FD数目有限制(默认1024)**
- 轮询时会**将所有fd从用户态复制到内核态**.随着fd量的增长,此复制开销也会线性增长
- 参数__FD_SETSIZE定义了每个FD_SET的句柄个数

#### poll

- 相比select，poll只是**取消了最大fd数限制**，并未从根本上解决select的问题

#### epoll 

- 只轮询就绪链表,适用于高并发长连接场景. **仅能用于linux2.6以上内核,mac用不了**
    -> 一个进程能打开的FD数限制=n*10W(系统内存nGB)  这个数目和系统内存关系很大
    -> 将socket加入epoll文件系统里file对象对应的红黑树，并在每个fd上注册回调函数，当某个阻塞态socket的IO完成时向CPU发出中断信号，内核异步唤醒并将该socket放入就绪队列，并从设备拷贝数据到内核

​    |内核缓冲区 -> 内核判断读/写缓冲区,判断fd可读还是可写.epoll只关心缓冲区非空/非满事件
​        |读缓冲区 -> sckt.recv() 为空则read返回-1, errno=EAGAIN; 否则生成可读事件
​        |写缓冲区 -> sckt.send(data)为满则write返回-1,errno=EAGAIN;否则生成可写事件
​        |EAGAIN含义
​            Linux环境下开发经常会碰到很多错误(设置errno)，其中EAGAIN是其中比较常见的一个错误(比如用在非阻塞操作中)。 从字面上来看，是提示再试一次。这个错误经常出现在当应用程序进行一些非阻塞(non-blocking)操作(对文件或socket)的时候。

​            例如，以 O_NONBLOCK的标志打开文件/socket/FIFO，如果你连续做read操作而没有数据可读。此时程序不会阻塞起来等待数据准备就绪返回，read函数会返回一个错误EAGAIN，提示你的应用程序现在没有数据可读请稍后再试。 又例如，当一个系统调用(比如fork)因为没有足够的资源(比如虚拟内存)而执行失败，返回EAGAIN提示其再调用一次(也许下次就能成功)

​        |IO操作 -> 可读(select.EPOLLIN)和可写(select.EPOLLOUT), 内核获取fd事件类型

​    |红黑树(RB-Tree) -> epoll用红黑树监听套接字,插入和删除性能较好,时间复杂度O(logN)

​    |新建实例 -> epoll_create在专属的内核缓冲区创建epoll实例
​    |监听连接 -> epoll_ctl将新连接fd, 内核缓冲区事件类型和回调函数注册进内核
​    |事件发生 -> 有报文发过来, 内核执行回调, 将对应fd放入就绪链表
​    |执行IO -> epoll_wait轮询就绪链表取出fds, 然后将对应的数据复制到用户态. 用户进程按照事件类型执行读操作或写操作
​    |连接关闭 -> 当socket被close掉后，epoll_ctl会自动从epoll中删除对应fd

​    -> epoll_wait调用ep_poll，当EventLink为空（无就绪fd）时挂起当前进程，直到EventLink不空时才被激活. epoll_wait返回时，实际上是把内核中的就绪链表中的fd发送给用户态，用户态处理事件的同时，内核更新并继续维护就绪链表

​    |边缘触发(Edge Triggered) -> Nginx-Epoll默认工作方式,更高效的通知模式,推荐ET模式
​        -> 只有在监视的文件句柄上发生事件的时候内核才会通知进程fd准备就绪,之后不再通知
​    |水平触发(level triggered) -> epoll默认工作方式,redis的默认工作方式
​        -> 没有任何操作,内核还是会一直通知你一个fd是否就绪,这种模式编程出错误可能性小一点.传统的select/poll都是这种模型的代表

​    |XXX内存映射(mmap)??? -> 内核与用户空间映射同一块内存
​        -> mmap将用户空间的一块地址和内核空间的一块地址映射到同一块物理内存地址（不管是用户空间还是内核空间都是虚拟地址，最终要通过地址映射映射到物理地址），使得这块物理内存对内核和对用户均可见，减少用户态和内核态之间的数据拷贝。内核可以直接看到epoll监听的句柄，效率高
​        -> epoll_wait获取就绪fd时,返回的并不是实际的描述符,而是一个代表就绪描述符的值,拿到这些值去epoll中依次取得相应的文件描述符即可,避免了复制大量文件描述符带来的开销