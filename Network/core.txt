1.一次完整的http请求全过程
    1.浏览器输入url
    2.DNS域名解析
    3.tcp3次握手建立连接
    4.发送http请求
    5.服务器程序按照请求生成响应结果返回给客户端(浏览器)(短连接服务器马上tcp四次挥手主动关闭连接)
    6.浏览器解析html代码，并请求其中的资源
    7.浏览器渲染并显示页面

2.tcp/udp的区别？tcp粘包是怎么回事,如何处理？udp有粘包吗？
    |保护消息边界 -> 把每条数据当作'独立的消息'传输,接收端一次只能接收一个数据包
    |流传输 -> '无保护消息边界',如果发送端连续发送数据,接收端可能一次收到多个数据包

- 数据包这个概念只有UDP才有，TCP是流
- UDP是没有流量控制的；快的发送者可以很容易地就淹没慢的接收者，导致接收方的UDP丢弃数据报

|TCP -> 基于连接(需三次握手建立),对系统资源要求较多,包头结构复杂,保证数据正确性和顺序
    -> TCP为了保证可靠传输,'减少额外开销'(每次发包都要验证),采用了'流传输',相对于面向消息的传输,可以减少包的发送量,减少开销

​    |TCP粘包 -> 只在'流传输'中出现
​    -> 客户端与服务器会维持一个连接(Channel),数据在连接不断开的情况下,可以持续不断地将多个数据包发往服务器.如果发送的数据包太小,那么会启用Nagle算法(可配置是否启用)'对较小的数据包进行合并'(因此'TCP的网络延迟比UDP的高')然后再发送(缓冲区满、超时或者包大小足够).这样服务器在接收到数据流的时候就无法区分哪些数据包是客户端自己分开发送的,就产生了粘包;如果服务器消息没有被及时从缓存区取走,下次在取数据的时候可能就会出现'一次取出多个数据包'的情况,导致粘包(确切来讲,对于基于TCP协议的应用,不应用包来描述,而应用流来描述)
​        |解决办法
​        1.对于发送方引起的粘包,用户可通过编程设置来避免,TCP提供了'强制数据立即传送'的指令push,服务器收到push指令后,将本段数据'即时发送'出去,而不必等待发送缓冲区满
​        2.对于接收方引起的粘包,可通过优化程序设计、精简接收进程工作量、提高接收进程优先级等措施,使其'及时接收'数据,从而尽量避免出现粘包现象
​        3.封装待传输数据包时,用固定'分隔符'作为结尾符(数据中不能含结尾符),这样我们接收到数据后,如果出现结尾符,即人为将粘包分开;如果包中没有出现结尾符,则认为是分包,等待下个包中出现结尾符时合并成一个完整的包(常用于文本传输数据,如采用/r/n之类的分隔符)
​        4.在数据包的固定位置封装'数据包的长度信息'(或可计算数据包总长度的信息),服务器接收到数据后,先是解析包长度,然后根据包长度截取数据包(常用于自定义协议中)

|UDP -> 无连接,系统资源需求少,包头结构简单,不保证数据正确性和顺序,面向'数据报'传输
    -> 作为无连接不可靠的传输协议(适合频繁发送较小包),有'保护消息边界','不会对数据包进行合并',数据直接发出去,因此'不会出现粘包',每一个数据包也都是完整的(数据+UDP头+IP头等等发一次数据封装一次)

3.time_wait是什么情况？出现过多的close_wait可能是什么原因？

​    |大量TIME_WAIT的场景 -> 主要是因为'短连接'. 对于基于TCP的HTTP协议，主动关闭TCP连接的是Server端，这样Server端会进入TIME_WAIT状态. 具体现象是对于一个处理大量短连接的服务器,如果是由服务器主动关闭客户端的连接,将导致服务器端存在大量的处于TIME_WAIT状态的socket, 甚至比处于Established状态下的socket多的多,TIME_WAIT一般持续1-4分钟, 会严重消耗服务器资源, 大量消耗客户端端口,甚至'耗尽可用的socket,停止服务'
​        -> 有时是代码中'未及时关闭http连接'占用了socket，导致不能正常访问

​        |解决方法
​            -> nginx打开和后端的'长连接'，设置proxy_http_version 1.1;和proxy_set_header Connection '';
​            -> Linux下开启tcp_tw_recycle+tcp_timestamps, 快速回收TIME_WAIT
​            -> Linux下开启tcp_tw_reuse+tcp_timestamps, TIME_WAIT状态的socket连接可以'被新连接重用'
​            -> tcp_max_tw_buckets'控制并发的TIME_WAIT数量'，默认值是180000。如果超过默认值，内核会把多的TIME_WAIT连接清掉，然后在日志里打一个警告。官网文档说这个选项只是为了阻止一些简单的DoS攻击，平常不要人为降低它
​            -> 向目标发送RST包清理TIME_WAIT(RST攻击)
​            -> 使用连接池

​            vi /etc/sysctl.conf
​            编辑文件，加入以下内容：
​            net.ipv4.tcp_syncookies = 1 #表示开启SYN Cookies。当出现SYN等待队列溢出时，启用cookies来处理，可防范少量SYN攻击，默认为0，表示关闭
​            net.ipv4.tcp_tw_reuse = 1 #表示开启重用。允许将TIME-WAIT sockets重新用于新的TCP连接，默认为0，表示关闭
​            net.ipv4.tcp_tw_recycle = 1 #表示开启TCP连接中TIME-WAIT sockets的快速回收，默认为0，表示关闭
​            net.ipv4.tcp_fin_timeout = 30 #修改系統默认的占用socket时长
​            然后执行 /sbin/sysctl -p 让参数生效

|大量close_wait -> 被动关闭方(接收到对方发过来FIN信号)未关闭socket造成
    |原因 -> 在服务器与客户端通信过程中,客户端关闭连接之后服务器未关闭socket导致closed_wait发生,致使监听port打开的句柄数到了1024个(每个用户默认分配的句柄上限),且均处于close_wait的状态.close_wait状态的socket需要先发送待处理的报文，然后才会关闭连接，这样的socket大量出现时，就会阻塞服务器.最终造成配置的'port被占满'出现“Too many open files”,无法进行通信
    -> linux在文件句柄的数目上有两个级别的限制('系统级限制和针对用户的限制').一般情况1024够用了,但是在频繁使用网络和文件IO的大容量系统上,socket很快就会耗光
    -> 有两种限制soft&hard,数目超过soft时会warning,达到hard时系统将拒绝或异常
    -> CLOSE_WAIT状态的连接在2个小时后也会自己关闭
    -> 有时是代码中'未及时关闭http连接(HttpWebResponse)'占用了socket，导致不能正常访问
    -> 如果一直保持在CLOSE_WAIT状态，那么只有一种情况，就是在对方关闭连接之后服务器程序自己没有进一步发出ack信号。换句话说，就是在对方连接关闭之后，程序里没有检测到，或者程序压根就忘记了这个时候需要关闭连接，于是这个资源就一直被程序占着。大量CLOSE_WAIT的解决办法总结为一句话就是：'查代码'。因为问题出在服务器程序里

​    |解决办法
​        1.设置超时 -> 调用ServerSocket类的accept()方法和Socket输入流的read()方法时会引起线程阻塞,所以应该用setSoTimeout()方法设置超时(缺省的设置是0,即超时永远不会发生);超时的判断是累计式的,一次设置后,每次调用引起的阻塞时间都从该值中扣除,直至另一次超时设置或有超时异常抛出
​        比如,某种服务需要三次调用read(),超时设置为1分钟,那么如果某次服务三次read()调用的总时间超过1分钟就会有异常抛出,如果要在同一个Socket上反复进行这种服务,就要在每次服务之前设置一次超时
​        2.修改配置
​            |增加 -> /etc/security/limits.conf #保存后reboot
​                username(* -> 所有用户) soft nofile 65535 #软限制
​                username hard nofile 65535 #硬限制

​            |系统级句柄数限制修改
​                sysctl -w fs.file-max 65536
​             or echo "65536" > /proc/sys/fs/file-max
​             or echo "fs.file-max=65536" >> /etc/sysctl.conf #修改内核参数

​            |优先级 -> soft<hard<kernel<最大fd数采用的数据结构导致的限制

​            #增加本地端口，以应对大量连接. 该参数指定端口的分配范围，该端口是向外访问的限制
​            $ echo ‘1024 65000′ > /proc/sys/net/ipv4/ip_local_port_range
​        3.使用'连接池'来控制连接数
​        4.确保'连接关闭'

