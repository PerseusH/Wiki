|进程fd -> 进程文件描述符

|挂起(动作) -> 由系统或程序主动执行,阻塞当前进程/线程,需主动唤醒(不释放CPU,可能释放内存,放在外存)
|睡眠(动作) -> 由系统或程序主动执行, 阻塞当前进程/线程，定时唤醒
|阻塞(状态) -> 被动执行，在抢占资源中得不到资源，被动的挂起在内存，等待某种资源或信号量（即有了资源）将他唤醒(释放CPU，不释放内存).进程/线程的'{阻塞和唤醒}'会导致CPU从'{用户态切换到内核态}'

-> 大数据处理的效率在于良好的分布计算逻辑(并行计算),而不是什么语言.'{瓶颈在IO}',使用SSD硬盘解决
-> *RPC*通常意味着进程是在和其它进程通信（可能是本地服务进程或者远程服务进程）

\死锁
    -> 假设有两个全局资源，a和b，有两个线程thread1，thread2. thread1占用a，想访问b，但此时thread2占用b，想访问a，两个线程都不释放此时拥有的资源，并互相等待，那么就会造成死锁
    -> 可抢占资源: CPU，主存等可共享的资源;不可抢占资源: 打印机,光驱等不可共享的资源(临界资源)

    |安全状态 -> 如果存在一个由所有进程构成的'{安全序列}'P1，…，Pn，则系统处于安全状态. 不存在安全序列则是不安全状态. 安全状态一定没有死锁发生
        |安全序列 -> 每个线程必须先获取第一个资源, 才能继续顺序获取下一个资源

    |必要条件
        1.互斥 -> 某资源只能被一个进程使用，其他进程请求该资源时只能等待，直到资源使用完后释放
        2.请求和保持 -> 程序已经占用了至少一个资源，但是又请求其他进程占用的资源
        3.不可抢占 -> 进程已获得的资源没有使用完，不能被抢占
        4.循环链 -> 必然存在一个循环链

    |解决方法 -> 保证所有线程按照一致的顺序(安全序列)获取资源
        1.单线程异步IO
        2.银行家算法

\Coroutine
    -> '{用户态}'的轻量级线程, 切换时不需要陷入系统调用(system call)(不会进入内核态)
    -> 事件驱动'{单线程}'的'{异步}'编程模型,'{没有切换开销}','{无需锁}',但无法利用多核资源
    -> Python对协程的支持是通过'{generator}'的'{yield + gen.send(n)}'实现的
    -> 在generator中,我们不但可以通过for循环来迭代,还可以不断调用next()函数获取由yield语句返回的下一个值或接收调用者发出的参数
    -> 协程拥有自己的本地寄存器'{上下文和栈}'，与其它协程共享全局数据和其它资源。协程切换时，将寄存器上下文和栈保存到其他地方(保存运行状态); 切回来的时候，恢复先前保存的寄存器上下文和栈
    -> 协程能保留上一次调用时的状态（即所有局部状态的一个特定组合），每次过程重入时，就相当于进入上一次调用的状态，换种说法：进入上一次离开时所处逻辑流的位置
    -> 多线程请求返回是无序的，哪个有数据返回就处理哪个，而协程程序是'{有序}'的，'{更容易调试}'
    -> 单线程执行，处理密集CPU和本地磁盘IO时性能较低。处理'{网络IO}'性能比较高
    -> 目前主流语言基本上都选择了多线程作为并发措施，与线程相关的概念是抢占式多任务（Preemptive multitasking），而与协程相关的是'{协作式多任务}'
    -> 不管是'{进程还是线程}'，每次阻塞、切换都需要陷入系统调用(system call)(用户态进入内核态)，因此上下文切换代价大.先让CPU跑操作系统的调度程序，然后再由调度程序决定该跑哪一个进程(线程)。而且由于抢占式调度'{执行顺序无法确定}'的特点，使用线程时需要非常小心地处理同步问题，而协程完全不存在这个问题（事件驱动和异步IO也有同样的优点）
    -> 协程是用户自己来编写调度逻辑的，因此'{同一时间只有一个}'协程在运行，相当于单线程的能力，所以CPU不用去考虑怎么调度、切换上下文，省去了CPU的切换开销，所以协程在一定程度上又好于多线程
    -> 协程没有开销的限制，理论上可以有'{无限个}'，把20W条url放在单进程的协程里执行，完全没问题
    -> '{多进程+协程}',既充分'{利用多核}',又充分发挥'{协程的高效率}',可获得极高的性能
    -> 调度协程有CPU开销,保存协程上下文有内存开销,性能'{不如事件驱动异步回调}'的编程模型
    -> 阻塞队列：最常见的业务场景就是生产者不断生产任务放进阻塞队列中，消费者不断从阻塞队列中获取任务；当阻塞队列中填满数据时，所有生产者端的线程自动阻塞，当阻塞队列中数据为空时，所有消费端的线程自动阻塞
    -> 非阻塞IO的本质是将用户程序和系统调用解耦

    |并发切换 -> IO->挂起当前过程->切换至其他过程运行->IO完毕,过程就绪->执行过程剩余代码
        |诱发动作
            1.disk IO
            2.network IO
            3.time.sleep(n)
            4.gevent.sleep(n)
            5.threading.Condition().wait()
            6.threading.lock().aquire()

    |gevent -> 早期gevent使用libevent，1.0版本之后替换成'libev'，因为libev“提供更少的核心功能以求更改的效率”。libev使用轮训非阻塞的方式进行事件处理，比如unix下的epoll。为各种并发和网络相关任务提供了整洁的API.主要模式是greenlet
        -> gevent来源于eventlet，自称比后者实现更简单、API更方便且性能更好，许多开源的web服务器也使用了gevent，如gunicorn、paste，当然gevent本生也可以作为一个python web服务器使用。这篇文章对常见的wsgi server进行性能对比，gevent不管在http1.0还是http1.1都表现非常出色
        |greenlet -> 以C扩展模块形式接入Python的轻量级协程。greenlet全部运行在主程序操作系统进程的内部，但它们被协作式地调度. 协程的上下文切换遇到阻塞时才会发生
        -> libevent也是对linux'{同步io}'做了封装，使它看起来像异步
        -> gevent实现了python标准库里面大部分的阻塞式系统调用，包括socket、ssl、threading和select等模块，而将这些阻塞式调用变为'{协作式运行}'（monkey.patch_xxx()）
        -> monkey-patch充分利用了动态语言的灵活性，'{不改变源代码}'而对功能进行追加和变更,可以对现有的语言Api进行'{追加，替换，修改Bug}'，甚至性能优化等等
        -> 通过monkey-patch的monkey.patch_xxx()将python标准库中模块或函数改成gevent中的响应的具有协程的协作式对象。这样在不改变原有代码的情况下，将python标准库的阻塞操作变成异步的
        -> monkey-patch在socket、ssl、threading和select等模块中所有可能进行IO操作的地方加了一个标记，相当于gevent.sleep(n)，打了补丁后，这些操作一开始执行就会阻塞并切换协程
        -> gevent面对CPU密集型以及本地IO密集型的任务都很无力
        -> 每一个greenlet.greenlet实例都有一个parent（可指定，默认为创建新的greenlet.greenlet所在环境），当greenlet.greenlet实例执行完逻辑正常结束、或者抛出异常结束时，执行逻辑切回到其parent

        |模式
            1.当前greenlet发生系统调用, socket阻塞
            2.挂起当前greenlet, 由main greenlet切换执行其他greenlet
            3.libev事件循环监听阻塞的socket
            4.IO完成, 内核将数据拷贝到libev事件循环
            5.main greenlet切换到就绪socket的greenlet继续执行

            在普通的greenlet中(非main greenlet)中，创建一个socket。 将socket风转到libev的io类型的watcher中，再将这个watcher和当前greenlet关联起来。 最后从当前greenlet切换到main greenlet。

            最后由libev的loop轮询检测socket上发生的IO事件，当事件发生后，将从main greenlet切换到刚才被挂起的greenlet继续执行。

            这样就体现了libev的异步和greenlet的同步完美的结合
            当从socket接收数据并发生阻塞事件时，为了能异步的得到内核发送的事件通知，将watcher加入到libev的loop循环中。

            self.hub.wait(self._read_event)就是将watcher加入到libev的loop循环中。

            调用hub.wait方法后，会从当前greenlet切换到main greenlet，也就是hub。过程如下所述

            由于hub管理着所有的greenlet，并将这些greenlet和libev的loop关联起来。这是通过libev的watcher来关联的。

            在hub.wait中，启动一个Waiter: waiter = Waiter(), 并将water.switch这个回调函数和watcher关联起来: watcher.start(waiter.switch, unique)。

            最后执行wait.get将当前greenlet切换到main greenlet。这时libev如果检测到socket上发生了greenlet感兴趣的事件，则从main greenlet切换到刚才被挂起的greenlet，并从挂起处继续执行

            Libev是一个事件循环：你注册感兴趣的特定事件（比如一个文件可以读取时或者发生超时时），它将管理这些事件源，将这些事件反馈给你的程序。在进程或线程中执行事件循环句柄控制，然后通过'回调机制'进行事件通信。通过watchers注册感兴趣的特定事件，这些watchers都是相对较小的C语言结构体，它们通过初始化具体的事件得到，然后交由libev启动那个watcher。Libev支持 select,poll,Linux特有的epoll,BSD特有的kqueue以及Solaris特有的文件描述符事件端口机制（ev_io），Linux信息通知接口（ev_stat），Linux事件文件/信号文件（为了更快更完整的唤醒沉睡线程（ev_async）/信号捕捉（ev_signal））相对定时器（ev_timer），用户自定义的绝对定时器（ev_periodic），同步信号（ev_signal），进程状态改变事件（ev_child），和通过事件循环机制实现的事件观察者管理本身（ev_idle,ev_embed,ev_prepare和ev_check监控）也和文件监控（ev_stat）和有限支持的派生子进程事件（ev_fork）一样

        |hub -> 底层greenlet任务阻塞时切换回hub, 然后由hub切换执行其他greenlet任务
        -> hub是gevent核心部件，依靠libev这个事件库，hub调用libev提供的事件循环来'调度'所有的任务greenlet
        -> hub也是一个greenlet，就是所谓的'main greenlet'
        -> hub会首先启动，然后马上启动事件循环。也就是'libev的loop'
        -> hub保存在线程的本地数据中，每个线程中只存在一个hub
        -> hub是线程内唯一的，greenlet是线程独立的，每个线程有各自的greenlet栈
        -> 从main greenlet切换到普通greenlet之后，main greenlet是停止执行(阻塞)的。greenlet是安排合理的'{串行}'，从而看起来像是并行
        -> 由hub决定运行哪个greenlet
        -> 切换到main greenlet是指首先切换到hub.wait，然后在hub.wait中切换到hub，hub运行loop, loop监听到greenlet感兴趣的事件(内核发送的事件, 如: IO完成, 定时器结束)后, 由hub.switch切换到发生事件的那个greenlet。主导整个过程的是libev的loop
        -> gevent.spawn(func)创建一个新的Greenlet，并将其注册到hub的loop上(func即该greenlet的run()方法)，调用gevent.joinall或者Greenlet.join的时候开始切换到hub, 由hub.switch切换到第一个greenlet开始执行self.run()
        -> run方法是hub的核心，由run方法来启动libev的loop run方法启动了libev的loop，并且会一直阻塞，直到收到了信号
        -> gevent.joinall就是用来启动事件轮询并等待运行结果的
            |核心API
            loop.timer(seconds, ref=ref) #定时器
            gevent.sleep(n) -> hub.wait(loop.timer(seconds, ref=ref))
            切换协程 -> hub.switch()
            hub.run() -> loop.run() #这个loop理论上会一直循环，如果结束，那么表明没有任何监听的事件（包括IO 定时等）

        |调度顺序
            1.libev的loop依次执行gevent.joinall(list)中的任务, 遇到阻塞就切换到下一个
            2.loop监听到greenlet感兴趣的事件(IO完成,定时器结束)后,切换到发生事件的greenlet

        > #执行到IO操作或gevent.sleep(n)阻塞时，gevent自动切换协程
        > from gevent import monkey; monkey.patch_socket() #monkey.patch_all()
        > import gevent
        >
        > def f(n):
        >     for i in range(n):
        >         print gevent.getcurrent(), i
        >
        > g1 = gevent.spawn(f, 5) #Greenlet对象, f本质是回调函数
        > g2 = gevent.spawn(f, 5)
        > g3 = gevent.spawn(f, 5)
        > g1.join() #join 就是在启动 gevent 的 gevent loop
        > g2.join()
        > g3.join()
        > #gevent.joinall([g1, g2, g3]) #会阻塞当前流程，并执行所有给定的greenlet，执行流程只会在所有greenlet执行完后才会继续向下走

\Process&Thread
    |GIL -> Global Interpreter Lock,全局解释器锁,解释器执行代码时,任何Python线程执行前,必须'{先获得GIL锁}',然后,每执行100条字节码,当前线程就会阻塞(time.sleep(n)),解释器就自动释放GIL锁,让别的线程有机会执行.这个GIL全局锁实际上把所有线程的执行代码都给上了锁,所以,'{多线程}'在Python中只能交替执行,即使100个线程跑在100核CPU上,也只能用到1个核,在官方解释器上'{不能用多核}'实现真正的多线程
    -> Python进程有各自独立的GIL锁

    -> 进程是并发执行的程序在执行过程中分配和管理资源的基本单位
    -> 进程有自己的内存空间，数据栈等，只能使用进程间通讯（InterProcessCommunication, IPC），不能直接共享信息. Linux中每个进程有两个栈，分别用于用户态和内核态的进程执行
    -> 线程是CPU调度的基本单位.软硬件资源的分配与线程无关，线程只能共享它所属进程的资源
    -> python里的线程是操作系统的真实线程
    -> 进程拥有一个完整的虚拟地址空间，不依赖于线程而独立存在；线程是进程的一部分，没有自己的地址空间，与进程内的其他线程一起共享分配给该进程的所有资源
    -> 每个进程都有进程控制表(PCB)，每个线程也有线程控制表(TCB)
    -> 进程切换只能发生在内核.内核通过一个唯一的进程标示值或PID来标识每一个进程
    -> 进程至少有 5 种基本状态，它们是：新建，执行，阻塞，就绪，终止
    -> 线程有 4 个基本状态：新建，执行，阻塞，终止.存在 5 种基本操作来切换线程的状态：派生，阻塞，唤醒，调度，结束
    -> 所有进程的祖先叫做进程0（idle进程），它是在linux的初始化阶段从无到有创建的一个内核进程，唯一使用静态分配数据结构的进程（所有其他进程的数据结构都是动态分配），所以这些静态分配的数据结构也顺其自然的变成链表的头
    ->  Linux创建进程分解到两个单独的函数中去执行：fork()和exec()。首先，fork()通过拷贝当前进程创建一个子进程，子进程与父进程的区别仅仅在于PID、PPID和某些资源和统计量。exec()函数负责读取可执行文件并载入地址空间开始运行. 进程终止的一般方式是调用exit()系统调用，即可能显式地调用，也可能隐式的从某个主函数返回；当进程接收到它既不能处理也不能忽略的信号或异常时，还可能被动终结。不管如何，在内核都是通过do_exit完成工作
    -> 内核经常需要在后台执行一些操作（如刷磁盘，空闲页回收等），这种任务可以交给内核线程。内核线程有以下特点
         1.没有独立的地址空间，所有内核线程共享地址空间
         2.只运行在内核态，从不切换到用户空间
         3.只能使用大于PAGE_OFFSET的线性地址空间
         4.可以被调度，可以被抢占

    |进程卡死(hanging挂起)原因: IO操作或并发过高, 一般是卡在某个系统调用上
        1.read很大的数据流, 等待对方输入
        2.IO请求的接收端(磁盘, 数据库)长时间没有反应, 进程等待
        3.消息队列数据量过大导致轮询进程卡死
        4.pipe缓冲区满, write阻塞等待; pipe缓冲区空, read阻塞等待

        |定位
            1.gdb -p <pid> > bt|backtrace|where -> 查看
            2.strace + pstack
            3.top查看CPU使用情况
            4.查看 /proc 文件系统内核态信息.检测/proc/<pid>目录是否可读，可判断进程是否存在
            5.查看/proc/<pid>/wchan 查看导致进程睡眠或者等待的函数
            6.查看/proc/<pid>/status 中上下文切换次数,多次查询后,若切换次数没有增加,说明进程完全卡死. 进程状态D(Disk Sleep)表示不可中断睡眠
            7.查看/proc/<pid>/stack 查看进程调用栈
            8.查看/proc/<pid>/syscall 找到进程挂在哪个系统调用上
            9.grep num /usr/include/asm-generic/unistd.h 查看syscall num对应的api

    |僵尸进程 -> 如果子进程先于父进程退出,同时父进程又没有调用wait/waitpid,则该子进程将成为僵尸进程。通过ps命令查看其带有defunct的标志。僵尸进程是一个早已死亡的进程，但在进程表 （processs table）中仍占了一个位置（slot）
        -> 要在当前进程中生成一个子进程，一般需要调用fork这个系统调用，fork这个函数的特别之处在于一次调用，两次返回，一次返回到父进程中，一次返回到子进程中，我们可以通过返回值来判断其返回点
        -> 为了防止产生僵尸进程，在fork子进程之后我们都要wait它们；同时，当子进程退出的时候，内核都会给父进程一个SIGCHLD信号，所以我们可以建立一个捕获SIGCHLD信号的信号处理函数，在函数体中调用wait（或waitpid），就可以清理退出的子进程以达到防止僵尸进程的目的

    |线程退出
        -> 线程结束运算时就会退出。线程可以调用thread.exit()之类的退出函数，也可以使用退出进程的标准方法sys.exit(n)或raise SystemExit(1)。不可以直接杀掉Kill一个线程
        -> 主线程应该是一个好的管理者,它要了解每个子线程都会做些什么,需要什么数据和参数，以及在线程结束时,它们生成了什么结果。这样就可以把各子线程的结果组成一个有意义的最终结果
        -> 不建议使用thread模块。主要原因是当主线程退出的时候，其他所有子线程没有结束就退出了。而threading模块就能确保所有“重要的”子线程都退出后，进程才会结束

    |'{线程不安全}' -> 每个线程互相独立，相互之间没有任何关系，但是在同一个进程中的线程，资源是共享的，如果不进行资源的合理分配，对数据造成破坏，使得线程运行的结果不可预期
        -> 线程本身的特点导致线程的适用范围是受限的.只有CPU过剩,而其它的任务很慢,此时用线程才是有益的,可以很好平衡等待时间,提高并发性能
        -> 线程的问题主要是线程的安全稳定性.线程无法强制中止,同时线程与主进程共享内存,可能会影响主进程的内存管理

    |计算密集型任务 -> 程序系统大部分在做计算、逻辑判断、循环导致cpu占用率很高的情况，比如计算圆周率、对视频进行高清解码等等，全靠'{CPU的运算能力}'
        -> 计算密集型任务由于主要消耗CPU资源，因此，代码运行效率至关重要。Python这样的脚本语言运行效率很低，完全不适合计算密集型任务。对于计算密集型任务，最好用C语言编写

    |IO密集型任务 -> 频繁网络传输、读取硬盘及其他设备，99%的时间都花在IO上('{经常阻塞}')
        -> IO密集型任务花在CPU上的时间很少，用运行速度极快的C语言替换用Python这样运行速度极低的脚本语言，完全无法提升运行效率。对于IO密集型任务，最合适的语言就是开发效率最高（代码量最少）的语言，脚本语言是首选，C语言最差
        -> 如果瓶颈是IO问题，CPU占用率应该很低，或者就是0%

    -> 当我们想实现多任务处理时,首先要想到使用'{multiprocessing}',但是如果觉着进程太笨重,那么就要考虑使用线程. 如果多任务处理中需要处理的太多了,可以考虑多进程,每个进程再采用多线程.如果还处理不要,就要使用轮询模式,比如使用poll event,twisted等方式.如果是GUI方式,则要通过事件机制,或者是消息机制处理,GUI使用单线程

    |多线程 -> 适用IO密集型任务. 在8核16线程服务器上测试最大可以同时开启4792个线程(600/核)
        -> 当一个任务阻塞在IO操作上时，可以立即切换到其他线程上执行其他IO操作
        -> 多线程模式一旦线程数目过多, CPU'{切换线程的开销}'也会很大

        |'{多线程的致命缺点}'->任何一个线程挂掉都可能造成整个进程崩溃,因为所有线程共享进程内存
            |解决方法 -> 子进程.子进程崩溃后,会完全的释放所有的内存和错误状态,所以更安全. 另外通过进程,python可以很好的绕过GIL,解决全局锁问题
            -> 线程不安全是事实.如果仅仅是做几个后台任务,则可以考虑使用守护线程做.如果需要做一些危险操作,可能会崩溃的,就用子进程去做. 如果需要高稳定性,同时并发数不高的服务,强烈建议用多进程multiprocessing模块实现.linux进程的使用代价低于windows,还可以接受

        -> 多线程还会造成死锁, 状态同步的问题

    |多进程 -> 适用计算密集型任务
        -> '{多进程}'能'{利用多核资源}'并行计算, 但是进程也是有局限的.'{不要建立超过CPU总核数}'的进程,否则效率也不高(任务越多，任务切换的时间就越多)

        |缺点 -> 多进程之间通信成本高，切换开销大

    |自旋锁 -> 使用一个while True循环, 不断尝试获取锁，直到成功

    |多进程
        > #多进程利用多核(4核CPU环境)
        > from multiprocessing import Process
        >
        > #dead loop
        > def loop():
        >     while 1: pass
        >
        > if __name__ == '__main__':
        >
        >     Process_list = []
        >     for i in range(3):
        >         p = Process(target=loop)
        >         p.start()
        >         Process_list.append(p)
        >
        >     #让主进程等待子进程执行完成
        >     for p in Process_list:
        >         p.join()
        >
        >     while 1: pass

    |多线程
        > import threading
        > import time
        >
        > local = threading.local() #隔离线程空间
        > #可以将ThreadLocal理解成一个dict,可以绑定不同变量.ThreadLocal用的最多的地方就是每一个线程处理一个HTTP请求，在Flask框架中利用的就是该原理，它使用的是基于Werkzeug的LocalStack
        >
        > def target():
        >     print 'current thread:%s' % threading.currentThread().name
        >     local.name = name
        >     print "%s in %s" % (local.name,threading.currentThread().name)
        >
        > print 'the curent threading  %s is running' % threading.current_thread().name
        > t1 = threading.Thread(target=target, args=('Bruce',))
        > t2 = threading.Thread(target=target, args=('Perseus',))
        > #为线程实例添加setDaemon(True)之后，如果不加join语句，那么当主线程结束之后，会杀死该线程.加上join,并设置等待时间t1.join(1)，就会等待线程一段时间再退出
        > t1.setDaemon(True) #其意义在于表示此线程不重要,默认情况下自动随主线程退出而退出
        > t2.setDaemon(True)
        > #所有的线程都创建之后，会一起调用start()函数启动线程，而不是创建一个启动一个
        > #join是阻塞当前线程，即在当前线程结束时，不会退出,会等到子线程结束，或者在给了timeout参数的时候，等到超时为止
        > #默认情况下，如果不加join语句，那么主线程会自然执行，但不会立即杀死子线程
        > t1.start()
        > t2.start()
        > t1.join()
        > t2.join()
        > print 'the curent threading  %s is ended' % threading.current_thread().name
        -----------------------------------------
        #线程锁(threading.Lock互斥锁. 有两个方法：acquire() and release())
        #acquire -> 无参数或参数为ture时，阻塞方式获得锁，即要等到锁释放后方能加锁，而后返回Ture; 有参数且参数设为false时，即为非阻塞方式获得锁。具体来说，如果已被锁定，直接返回false，不等待；如果未被锁定，则返回ture
        #release -> 释放锁
        > a = 3
        > lock = threading.Lock()
        > #with lock: #do something
        > def target():
        >     print 'the curent threading  %s is running' % threading.current_thread().name
        >     time.sleep(4) #阻塞并挂起当前线程,系统切换到下一个线程运行
        >     global a
        >     lock.acquire() #锁住资源, 其他请求锁的线程阻塞等待
        >     try:
        >         a += 3
        >     finally:
        >         lock.release() #释放资源
        >     print 'the curent threading  %s is ended' % threading.current_thread().name
        >     print 'yes'
        -----------------------------------------
        #条件变量
        #线程A需要等某个条件成立才能继续往下执行，现在这个条件不成立，线程A就阻塞等待，而线程B在执行过程中使这个条件成立了，就唤醒线程A继续执行。在pthread库中通过条件变量（Condition Variable）来阻塞等待一个条件，或者唤醒等待这个条件的线程。

        #通俗的讲，生产者，消费者的模型。 condition很适合那种主动休眠，被动唤醒的场景。 condition使用难度要高于mutex，一不注意就会被死锁，SO一定要理解condition实现后再用

        #一个Condition实例的内部实际上维护了两个队列，一个是等待锁队列，mutex内部其实就是维护了一个队列。 另一个队列可以叫等待条件队列，在这队列中的节点都是由于（某些条件不满足）线程自身调用wait方法阻塞的线程，记住是自身阻塞。最重要的Condition方法是wait和 notify方法。另外condition还需要lock的支持， 如果你构造函数没有指定lock，condition会默认给你配一个rlock
        > condition=threading.Condition()
        > while 1:
        >    if condition.acquire():  #获取锁
        >        if len(task) > 1000:
        >            condition.wait() #条件不满足则一直阻塞等待
        > #                           1.加入到等待条件队列（注意这里不是等待锁的队列）
        > #                           2.释放锁
        > #                           3.阻塞自身线程
        > #                           ————条件满足, 被唤醒后执行————-
        > #                           4.尝试获取锁（这时线程已不在等待条件队列中，而是位于等待锁队列中，参见signal(notify)方法）
        > #                               4.1成功,从wait方法中返回，执行线程后面的代码
        > #                               4.2失败,阻塞自己（等待前一个节点释放锁时将它唤醒）
        > #注意：调用wait可以让当前线程休眠，等待其他线程的唤醒，也就是等待signal(notify)，这个过程是阻塞的。 当队列首线程被唤醒后，会继续执行wait方法后面的代码
        >        else:
        >            #doing task
        >            condition.notify() #唤醒wait队列的线程
        >                               #1.取出等待条件队列的首节点,放入等待锁队列的队尾
        >                               #2.唤醒节点对应的线程
        >        time.sleep(5)
        >        condition.release()
        |Socket

        |可重入锁 -> 为了支持在同一线程中多次请求同一资源，python提供了“可重入锁”：threading.RLock。RLock内部维护着一个Lock和一个counter变量，counter记录了acquire的次数，从而使得资源可以被多次require。直到一个线程所有的acquire都被release，其他的线程才能获得资源
